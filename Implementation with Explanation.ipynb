{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6ae69f59",
   "metadata": {},
   "source": [
    "# AI Model to Understand and Process Abstract Concepts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f35f2582",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "In this notebook, we will implement an AI model that can classify text based on abstract concepts like emotions (happiness and sadness). We will use a combination of Natural Language Processing (NLP) techniques and machine learning to achieve this."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8827ffae",
   "metadata": {},
   "source": [
    "## Step 1: Setup Environment\n",
    "First, ensure that you have the necessary libraries installed. Run the following commands to install `transformers`, `scikit-learn`, and `torch` if you haven't already."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f7a38c9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\91720\\anaconda3\\lib\\site-packages (4.32.1)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\91720\\anaconda3\\lib\\site-packages (1.3.0)\n",
      "Requirement already satisfied: torch in c:\\users\\91720\\anaconda3\\lib\\site-packages (2.4.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\91720\\anaconda3\\lib\\site-packages (from transformers) (3.9.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.15.1 in c:\\users\\91720\\anaconda3\\lib\\site-packages (from transformers) (0.15.1)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\91720\\anaconda3\\lib\\site-packages (from transformers) (1.24.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\91720\\anaconda3\\lib\\site-packages (from transformers) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\91720\\anaconda3\\lib\\site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\91720\\anaconda3\\lib\\site-packages (from transformers) (2022.7.9)\n",
      "Requirement already satisfied: requests in c:\\users\\91720\\anaconda3\\lib\\site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in c:\\users\\91720\\anaconda3\\lib\\site-packages (from transformers) (0.13.2)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in c:\\users\\91720\\anaconda3\\lib\\site-packages (from transformers) (0.3.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\91720\\anaconda3\\lib\\site-packages (from transformers) (4.65.0)\n",
      "Requirement already satisfied: scipy>=1.5.0 in c:\\users\\91720\\anaconda3\\lib\\site-packages (from scikit-learn) (1.10.1)\n",
      "Requirement already satisfied: joblib>=1.1.1 in c:\\users\\91720\\anaconda3\\lib\\site-packages (from scikit-learn) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\91720\\anaconda3\\lib\\site-packages (from scikit-learn) (2.2.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\91720\\anaconda3\\lib\\site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: sympy in c:\\users\\91720\\anaconda3\\lib\\site-packages (from torch) (1.11.1)\n",
      "Requirement already satisfied: networkx in c:\\users\\91720\\anaconda3\\lib\\site-packages (from torch) (3.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\91720\\anaconda3\\lib\\site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: fsspec in c:\\users\\91720\\anaconda3\\lib\\site-packages (from torch) (2023.4.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\91720\\anaconda3\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\91720\\anaconda3\\lib\\site-packages (from jinja2->torch) (2.1.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\91720\\anaconda3\\lib\\site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\91720\\anaconda3\\lib\\site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\91720\\anaconda3\\lib\\site-packages (from requests->transformers) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\91720\\anaconda3\\lib\\site-packages (from requests->transformers) (2024.6.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\91720\\anaconda3\\lib\\site-packages (from sympy->torch) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers scikit-learn torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fdf5137",
   "metadata": {},
   "source": [
    "## Step 2: Import Libraries\n",
    "We will import the necessary libraries for our implementation. These include `numpy` for numerical operations, `sklearn` for machine learning tasks, `transformers` for BERT model and tokenizer, and `torch` for handling tensor operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ee48f5f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\91720\\anaconda3\\Lib\\site-packages\\transformers\\utils\\generic.py:260: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff89a72d",
   "metadata": {},
   "source": [
    "## Step 3: Create a Dataset\n",
    "We will create a dataset of sentences labeled as expressing either happiness or sadness. This dataset will be used to train and evaluate our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "979e672e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [\n",
    "    (\"I feel great today!\", \"happiness\"),\n",
    "    (\"This is the best day ever!\", \"happiness\"),\n",
    "    (\"I am so sad and depressed.\", \"sadness\"),\n",
    "    (\"Why does everything bad happen to me?\", \"sadness\"),\n",
    "    (\"I am thrilled with the news!\", \"happiness\"),\n",
    "    (\"I can't stop crying.\", \"sadness\"),\n",
    "    (\"What a wonderful world!\", \"happiness\"),\n",
    "    (\"I am feeling blue.\", \"sadness\"),\n",
    "    (\"Everything is going well.\", \"happiness\"),\n",
    "    (\"I lost my job today.\", \"sadness\"),\n",
    "    (\"I am excited about the future.\", \"happiness\"),\n",
    "    (\"I am heartbroken.\", \"sadness\"),\n",
    "    (\"Life is beautiful.\", \"happiness\"),\n",
    "    (\"I am in despair.\", \"sadness\"),\n",
    "    (\"The sun is shining bright.\", \"happiness\"),\n",
    "    (\"I feel lonely.\", \"sadness\"),\n",
    "    (\"I have achieved my goals.\", \"happiness\"),\n",
    "    (\"I am worried about the test.\", \"sadness\"),\n",
    "    (\"I love spending time with my family.\", \"happiness\"),\n",
    "    (\"I am scared of the dark.\", \"sadness\"),\n",
    "    (\"I am so proud of myself.\", \"happiness\"),\n",
    "    (\"I regret my decisions.\", \"sadness\"),\n",
    "    (\"This is an amazing experience!\", \"happiness\"),\n",
    "    (\"I am feeling hopeless.\", \"sadness\"),\n",
    "    (\"I am on top of the world!\", \"happiness\"),\n",
    "    (\"I am completely lost.\", \"sadness\"),\n",
    "    (\"I am grateful for everything.\", \"happiness\"),\n",
    "    (\"I feel like a failure.\", \"sadness\"),\n",
    "    (\"Today is a fantastic day!\", \"happiness\"),\n",
    "    (\"I am disappointed.\", \"sadness\"),\n",
    "    (\"I am full of energy.\", \"happiness\"),\n",
    "    (\"I am exhausted.\", \"sadness\"),\n",
    "    (\"I am enjoying every moment.\", \"happiness\"),\n",
    "    (\"I am devastated.\", \"sadness\"),\n",
    "    (\"I am content with my life.\", \"happiness\"),\n",
    "    (\"I feel numb.\", \"sadness\"),\n",
    "    (\"I am in love.\", \"happiness\"),\n",
    "    (\"I am anxious.\", \"sadness\"),\n",
    "    (\"I am proud of my achievements.\", \"happiness\"),\n",
    "    (\"I feel empty inside.\", \"sadness\"),\n",
    "    (\"I am celebrating my success.\", \"happiness\"),\n",
    "    (\"I feel abandoned.\", \"sadness\")\n",
    "]\n",
    "\n",
    "sentences, labels = zip(*data)\n",
    "labels = np.array([1 if label == \"happiness\" else 0 for label in labels])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ead65218",
   "metadata": {},
   "source": [
    "## Step 4: Tokenize and Extract Features using BERT\n",
    "We will use BERT (Bidirectional Encoder Representations from Transformers) to tokenize the sentences and extract meaningful features. BERT is a state-of-the-art model for NLP tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a3a9c2c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "tokens = tokenizer(list(sentences), return_tensors='pt', padding=True, truncation=True, max_length=128)\n",
    "\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "with torch.no_grad():\n",
    "    outputs = model(**tokens)\n",
    "    features = outputs.last_hidden_state.mean(dim=1).numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d51a63e",
   "metadata": {},
   "source": [
    "## Step 5: Train a Classifier\n",
    "We will split the data into training and test sets and then train a logistic regression classifier on the features extracted by BERT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "70f55d3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "classifier = LogisticRegression()\n",
    "classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1d2c948",
   "metadata": {},
   "source": [
    "## Step 6: Evaluate the Model\n",
    "We will evaluate the accuracy of our model on the test set to see how well it performs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fca0eae0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 100.00%\n"
     ]
    }
   ],
   "source": [
    "accuracy = classifier.score(X_test, y_test)\n",
    "print(f'Accuracy: {accuracy * 100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a25c7aea",
   "metadata": {},
   "source": [
    "## Step 7: Make Predictions\n",
    "Finally, we will demonstrate making a prediction on a new sentence to classify it as expressing either happiness or sadness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cf9c9617",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The sentence \"I am extremely happy today!\" is classified as: happiness\n"
     ]
    }
   ],
   "source": [
    "test_sentence = \"I am extremely happy today!\"\n",
    "test_tokens = tokenizer(test_sentence, return_tensors='pt', padding=True, truncation=True, max_length=128)\n",
    "with torch.no_grad():\n",
    "    test_features = model(**test_tokens).last_hidden_state.mean(dim=1).numpy()\n",
    "prediction = classifier.predict(test_features)\n",
    "\n",
    "print(f'The sentence \"{test_sentence}\" is classified as: {\"happiness\" if prediction == 1 else \"sadness\"}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc6f5f22",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "In this notebook, we implemented an AI model to classify text based on abstract concepts like emotions using BERT for feature extraction and logistic regression for classification. This approach can be extended to more complex and larger datasets for improved accuracy and performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "95eb6f75-fe07-4ee1-84b5-e9647680462c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The sentence \"I am not well\" is classified as: sadness\n"
     ]
    }
   ],
   "source": [
    "test_sentence = \"I am not well\"\n",
    "test_tokens = tokenizer(test_sentence, return_tensors='pt', padding=True, truncation=True, max_length=128)\n",
    "with torch.no_grad():\n",
    "    test_features = model(**test_tokens).last_hidden_state.mean(dim=1).numpy()\n",
    "prediction = classifier.predict(test_features)\n",
    "\n",
    "print(f'The sentence \"{test_sentence}\" is classified as: {\"happiness\" if prediction == 1 else \"sadness\"}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "03722454-496b-4f42-ba51-0b435a4cd94a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The sentence \"I am good\" is classified as: happiness\n"
     ]
    }
   ],
   "source": [
    "test_sentence = \"I am good\"\n",
    "test_tokens = tokenizer(test_sentence, return_tensors='pt', padding=True, truncation=True, max_length=128)\n",
    "with torch.no_grad():\n",
    "    test_features = model(**test_tokens).last_hidden_state.mean(dim=1).numpy()\n",
    "prediction = classifier.predict(test_features)\n",
    "\n",
    "print(f'The sentence \"{test_sentence}\" is classified as: {\"happiness\" if prediction == 1 else \"sadness\"}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4890091f-9be0-464e-819c-6172867a0bb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The sentence \"I feel low\" is classified as: sadness\n"
     ]
    }
   ],
   "source": [
    "test_sentence = \"I feel low\"\n",
    "test_tokens = tokenizer(test_sentence, return_tensors='pt', padding=True, truncation=True, max_length=128)\n",
    "with torch.no_grad():\n",
    "    test_features = model(**test_tokens).last_hidden_state.mean(dim=1).numpy()\n",
    "prediction = classifier.predict(test_features)\n",
    "\n",
    "print(f'The sentence \"{test_sentence}\" is classified as: {\"happiness\" if prediction == 1 else \"sadness\"}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
